{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28",
      "mount_file_id": "1AsVA0ZAljb-AxDsFd6WoPQq5igduJHvO",
      "authorship_tag": "ABX9TyPAOPJ5K2dhIM9JzLg/mG1n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sappw1/Dissertation/blob/main/Notebooks/Notebooks-Working/ETL/PPP_Loan_EDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uaZbsG02n_v2"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import hashlib\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ppp_data_raw = pd.read_csv(\"/content/drive/MyDrive/NCU/Dissertation/Data/PPP_Loan_apps.csv\")\n",
        "ppp_data_raw"
      ],
      "metadata": {
        "id": "vsTJlkNMoDBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UKg0AwziFtgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nan_counts = ppp_data.isna().sum()\n",
        "print(nan_counts)"
      ],
      "metadata": {
        "id": "-zHnboj9rGFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "distinct_loan_status = ppp_data['ProcessingMethod'].unique()\n",
        "print(distinct_loan_status)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "JoWO9Csyj6VR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "generate a sample known fraud dataset"
      ],
      "metadata": {
        "id": "-UHj97nulc_I"
      }
    },
    {
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "num_records = 100\n",
        "\n",
        "known_fraud_cases = {\n",
        "    'LoanNumber': np.random.randint(1000000000, 9999999999, size=num_records),\n",
        "    'DateApproved': pd.to_datetime(np.random.choice(pd.date_range('2020-01-01', '2021-12-31'), size=num_records)).strftime('%Y-%m-%d'),\n",
        "    'BorrowerName': ['Company' + str(i) for i in range(1, num_records + 1)],\n",
        "    'FraudAmount': np.random.randint(100000, 500000, size=num_records),\n",
        "    'LoanStatus': np.random.choice(['Charged Off', 'Paid in Full', 'Exempt'], size=num_records),\n",
        "    'FraudDescription': np.random.choice(['Submitted false payroll information', 'Created a shell company to receive loan', 'Misused funds for personal expenses', 'Other fraudulent activity'], size=num_records),\n",
        "    'Source': np.random.choice(['DOJ', 'SBA OIG', 'PRAC', 'Other'], size=num_records)\n",
        "}\n",
        "\n",
        "known_fraud_cases = pd.DataFrame(known_fraud_cases)\n",
        "print(known_fraud_cases)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "BJm6azJxkjcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Data Collection - Loading PPP Data and Known Fraud Cases\n",
        "Weâ€™ll first load both the PPP dataset and the known_fraud_cases.csv file created from official sources."
      ],
      "metadata": {
        "id": "S0ANnl7GnVhb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add fraud label to known fraud cases\n",
        "known_fraud_cases['fraud_label'] = 1  # Mark known fraud cases as fraudulent\n",
        "ppp_data = ppp_data_raw.copy()\n",
        "# Mark all other cases in the main dataset as non-fraudulent initially\n",
        "ppp_data['fraud_label'] = 0  # Initial assumption of non-fraud for main data\n",
        "\n",
        "# Combine the datasets for integrated analysis semi-supervised, make a copy of unlabeled data for unsupervised\n",
        "ppp_supervised = pd.concat([ppp_data, known_fraud_cases], ignore_index=True)\n",
        "ppp_unsupervised = ppp_data.copy()"
      ],
      "metadata": {
        "id": "LnG8aYcilz8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: \\Preprocessing for unspervised Learning: Combined Dataset\n",
        "The combined dataset includes known fraud cases and general PPP data. Our focus here is on ensuring that critical fields are preserved, categorical fields are encoded, and any essential fields with missing values are handled in alignment with Chapter 3."
      ],
      "metadata": {
        "id": "0TxiFFEZnM15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Hash PII Fields (BorrowerName, BorrowerAddress, FranchiseName, ServicingLenderAddress, ServicingLenderName, OriginatingLender)\n",
        "def hash_column(df, column):\n",
        "    df[column] = df[column].apply(lambda x: int(hashlib.sha256(str(x).encode()).hexdigest(), 16) % (10 ** 8))\n",
        "    return df\n",
        "\n",
        "# Hash specified PII columns\n",
        "pii_columns = ['BorrowerName', 'BorrowerAddress', 'FranchiseName', 'ServicingLenderAddress', 'ServicingLenderName', 'OriginatingLender']\n",
        "for col in pii_columns:\n",
        "    ppp_unsupervised = hash_column(ppp_unsupervised, col)\n",
        "\n",
        "# Step 2: Convert Date Columns to Year, Month, Day Features (DateApproved, LoanStatusDate, ForgivenessDate)\n",
        "date_columns = ['DateApproved', 'LoanStatusDate', 'ForgivenessDate']\n",
        "for date_col in date_columns:\n",
        "    ppp_unsupervised[date_col] = pd.to_datetime(ppp_unsupervised[date_col], errors='coerce')  # Convert invalid dates to NaT\n",
        "    ppp_unsupervised[f'{date_col}_year'] = ppp_unsupervised[date_col].dt.year.fillna(0).astype(int)\n",
        "    ppp_unsupervised[f'{date_col}_month'] = ppp_unsupervised[date_col].dt.month.fillna(0).astype(int)\n",
        "    ppp_unsupervised[f'{date_col}_day'] = ppp_unsupervised[date_col].dt.day.fillna(0).astype(int)\n",
        "    ppp_unsupervised.drop(columns=[date_col], inplace=True)  # Drop original date column\n",
        "\n",
        "# Step 3: Label Encode Categorical Columns\n",
        "label_encode_cols = [\n",
        "    'BorrowerCity', 'BorrowerState', 'LoanStatus', 'ServicingLenderCity', 'ServicingLenderState',\n",
        "    'BusinessAgeDescription', 'ProjectCity', 'ProjectCountyName', 'ProjectState', 'BusinessType',\n",
        "    'OriginatingLenderCity', 'OriginatingLenderState', 'Race', 'Ethnicity'\n",
        "]\n",
        "\n",
        "label_encoders = {}\n",
        "for col in label_encode_cols:\n",
        "    le = LabelEncoder()\n",
        "    ppp_unsupervised[col] = le.fit_transform(ppp_unsupervised[col].astype(str))\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# Step 4: One-Hot Encode Binary Columns\n",
        "one_hot_cols = [\n",
        "    'ProcessingMethod', 'RuralUrbanIndicator', 'HubzoneIndicator', 'LMIIndicator', 'Gender',\n",
        "    'Veteran', 'NonProfit'\n",
        "]\n",
        "ppp_unsupervised = pd.get_dummies(ppp_unsupervised, columns=one_hot_cols, drop_first=True)\n",
        "\n",
        "# Step 5: Mode Impute for NAICSCode\n",
        "ppp_unsupervised['NAICSCode'] = ppp_unsupervised['NAICSCode'].fillna(ppp_unsupervised['NAICSCode'].mode()[0])\n",
        "\n",
        "# Step 6: Median Impute for Financial and Numeric Columns\n",
        "numeric_cols = [\n",
        "    'CurrentApprovalAmount', 'InitialApprovalAmount', 'JobsReported', 'ForgivenessAmount',\n",
        "    'UTILITIES_PROCEED', 'PAYROLL_PROCEED', 'MORTGAGE_INTEREST_PROCEED', 'RENT_PROCEED',\n",
        "    'REFINANCE_EIDL_PROCEED', 'HEALTH_CARE_PROCEED', 'DEBT_INTEREST_PROCEED', 'UndisbursedAmount'\n",
        "]\n",
        "for col in numeric_cols:\n",
        "    ppp_unsupervised[col] = ppp_unsupervised[col].fillna(ppp_unsupervised[col].median())\n",
        "\n",
        "# Step 7: Convert Location Identifiers to Numeric (BorrowerZip, ServicingLenderZip, ProjectZip, CD)\n",
        "location_cols = ['BorrowerZip', 'ServicingLenderZip', 'ProjectZip', 'CD']\n",
        "for col in location_cols:\n",
        "    ppp_unsupervised[col] = pd.to_numeric(ppp_unsupervised[col], errors='coerce')\n",
        "    ppp_unsupervised[col] = ppp_unsupervised[col].fillna(0).astype(int)  # Convert NaNs to 0 and cast to integer\n",
        "\n",
        "# Step 8: Derived Column - ForgivenessAmountRatio\n",
        "ppp_unsupervised['ForgivenessAmountRatio'] = ppp_unsupervised['ForgivenessAmount'] / ppp_unsupervised['CurrentApprovalAmount']\n",
        "ppp_unsupervised['ForgivenessAmountRatio'].fillna(0, inplace=True)  # Fill NaNs with 0 for any division by zero cases\n",
        "\n",
        "# Step 9: Verify No Missing Values\n",
        "missing_values = ppp_unsupervised.isnull().sum().sum()\n",
        "print(\"Total Missing Values after Preprocessing:\", missing_values)\n",
        "\n",
        "# Retain LoanNumber and SBAOfficeCode without modification\n",
        "\n",
        "print(\"Processed ppp_data sample:\\n\", ppp_unsupervised.head())\n",
        "print(\"Processed data column types:\\n\", ppp_unsupervised.dtypes)\n"
      ],
      "metadata": {
        "id": "0SLHIB4l4eRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing for semi-upervised Learning"
      ],
      "metadata": {
        "id": "wUY671eRv4dH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Hash PII Fields (BorrowerName, BorrowerAddress, FranchiseName, ServicingLenderAddress, ServicingLenderName, OriginatingLender)\n",
        "def hash_column(df, column):\n",
        "    df[column] = df[column].apply(lambda x: int(hashlib.sha256(str(x).encode()).hexdigest(), 16) % (10 ** 8))\n",
        "    return df\n",
        "\n",
        "# Hash specified PII columns\n",
        "pii_columns = ['BorrowerName', 'BorrowerAddress', 'FranchiseName', 'ServicingLenderAddress', 'ServicingLenderName', 'OriginatingLender']\n",
        "for col in pii_columns:\n",
        "    ppp_supervised = hash_column(ppp_supervised, col)\n",
        "\n",
        "# Step 2: Convert Date Columns to Year, Month, Day Features (DateApproved, LoanStatusDate, ForgivenessDate)\n",
        "date_columns = ['DateApproved', 'LoanStatusDate', 'ForgivenessDate']\n",
        "for date_col in date_columns:\n",
        "    ppp_supervised[date_col] = pd.to_datetime(ppp_supervised[date_col], errors='coerce')  # Convert invalid dates to NaT\n",
        "    ppp_supervised[f'{date_col}_year'] = ppp_supervised[date_col].dt.year.fillna(0).astype(int)\n",
        "    ppp_supervised[f'{date_col}_month'] = ppp_supervised[date_col].dt.month.fillna(0).astype(int)\n",
        "    ppp_supervised[f'{date_col}_day'] = ppp_supervised[date_col].dt.day.fillna(0).astype(int)\n",
        "    ppp_supervised.drop(columns=[date_col], inplace=True)  # Drop original date column\n",
        "\n",
        "# Step 3: Label Encode Categorical Columns\n",
        "label_encode_cols = [\n",
        "    'BorrowerCity', 'BorrowerState', 'LoanStatus', 'ServicingLenderCity', 'ServicingLenderState',\n",
        "    'BusinessAgeDescription', 'ProjectCity', 'ProjectCountyName', 'ProjectState', 'BusinessType',\n",
        "    'OriginatingLenderCity', 'OriginatingLenderState', 'Race', 'Ethnicity'\n",
        "]\n",
        "\n",
        "label_encoders = {}\n",
        "for col in label_encode_cols:\n",
        "    le = LabelEncoder()\n",
        "    ppp_supervised[col] = le.fit_transform(ppp_supervised[col].astype(str))\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# Step 4: One-Hot Encode Binary Columns\n",
        "one_hot_cols = [\n",
        "    'ProcessingMethod', 'RuralUrbanIndicator', 'HubzoneIndicator', 'LMIIndicator', 'Gender',\n",
        "    'Veteran', 'NonProfit'\n",
        "]\n",
        "ppp_supervised = pd.get_dummies(ppp_supervised, columns=one_hot_cols, drop_first=True)\n",
        "\n",
        "# Step 5: Mode Impute for NAICSCode\n",
        "ppp_supervised['NAICSCode'] = ppp_supervised['NAICSCode'].fillna(ppp_supervised['NAICSCode'].mode()[0])\n",
        "\n",
        "# Step 6: Median Impute for Financial and Numeric Columns\n",
        "numeric_cols = [\n",
        "    'CurrentApprovalAmount', 'InitialApprovalAmount', 'JobsReported', 'ForgivenessAmount',\n",
        "    'UTILITIES_PROCEED', 'PAYROLL_PROCEED', 'MORTGAGE_INTEREST_PROCEED', 'RENT_PROCEED',\n",
        "    'REFINANCE_EIDL_PROCEED', 'HEALTH_CARE_PROCEED', 'DEBT_INTEREST_PROCEED', 'UndisbursedAmount'\n",
        "]\n",
        "for col in numeric_cols:\n",
        "    ppp_supervised[col] = ppp_supervised[col].fillna(ppp_supervised[col].median())\n",
        "\n",
        "# Step 7: Convert Location Identifiers to Numeric (BorrowerZip, ServicingLenderZip, ProjectZip, CD)\n",
        "location_cols = ['BorrowerZip', 'ServicingLenderZip', 'ProjectZip', 'CD']\n",
        "for col in location_cols:\n",
        "    ppp_supervised[col] = pd.to_numeric(ppp_supervised[col], errors='coerce')\n",
        "    ppp_supervised[col] = ppp_supervised[col].fillna(0).astype(int)  # Convert NaNs to 0 and cast to integer\n",
        "\n",
        "# Step 8: Derived Column - ForgivenessAmountRatio\n",
        "ppp_supervised['ForgivenessAmountRatio'] = ppp_supervised['ForgivenessAmount'] / ppp_supervised['CurrentApprovalAmount']\n",
        "ppp_supervised['ForgivenessAmountRatio'].fillna(0, inplace=True)  # Fill NaNs with 0 for any division by zero cases\n",
        "\n",
        "# Step 9: Verify No Missing Values\n",
        "missing_values = ppp_supervised.isnull().sum().sum()\n",
        "print(\"Total Missing Values after Preprocessing:\", missing_values)\n",
        "\n",
        "# Retain LoanNumber and SBAOfficeCode without modification\n",
        "\n",
        "print(\"Processed ppp_data sample:\\n\", ppp_supervised.head())\n",
        "print(\"Processed data column types:\\n\", ppp_supervised.dtypes)\n"
      ],
      "metadata": {
        "id": "sPGlTsZY468N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "unsupervised learning\n",
        "\n"
      ],
      "metadata": {
        "id": "U3l9NexfCkOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load ppp_unsupervised data, excluding the fraud label\n",
        "X_unsupervised = ppp_unsupervised.drop(columns=['fraud'], errors='ignore')"
      ],
      "metadata": {
        "id": "7a5jvbc7Csls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Fit PCA with all components to find the optimal number based on explained variance\n",
        "pca_full = PCA()\n",
        "pca_full.fit(X_unsupervised)\n",
        "\n",
        "# Calculate cumulative explained variance\n",
        "explained_variance_ratio = np.cumsum(pca_full.explained_variance_ratio_)\n",
        "\n",
        "# Plot cumulative explained variance to find the \"elbow\"\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, marker='o')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('PCA Cumulative Explained Variance')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Determine optimal number of components to retain 98% variance\n",
        "optimal_components = np.argmax(explained_variance_ratio >= 0.98) + 1\n",
        "print(f\"Optimal number of components to retain 98% variance: {optimal_components}\")"
      ],
      "metadata": {
        "id": "VgFoT4yMC2vI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Apply PCA with the determined optimal number of components\n",
        "pca = PCA(n_components=optimal_components)\n",
        "X_reduced = pca.fit_transform(X_unsupervised)"
      ],
      "metadata": {
        "id": "fXGNpWUPDEhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: K-Means Clustering with Optimal K (Elbow Method)\n",
        "sum_of_squared_distances = []\n",
        "K_range = range(1, 11)  # Test K values from 1 to 10\n",
        "\n",
        "for K in K_range:\n",
        "    kmeans = KMeans(n_clusters=K, random_state=42)\n",
        "    kmeans.fit(X_reduced)\n",
        "    sum_of_squared_distances.append(kmeans.inertia_)\n",
        "\n",
        "# Plot Elbow Curve for K\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(K_range, sum_of_squared_distances, marker='o')\n",
        "plt.title('Elbow Method for Optimal K in K-Means')\n",
        "plt.xlabel('Number of Clusters (K)')\n",
        "plt.ylabel('Sum of Squared Distances (Inertia)')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2oTuY4rXDE_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Based on the Elbow Plot, select optimal K (e.g., K=4 if the elbow is observed there)\n",
        "optimal_K = 10  # Update this based on the elbow plot observation\n",
        "\n",
        "# Run K-Means with optimal K\n",
        "kmeans = KMeans(n_clusters=optimal_K, random_state=42)\n",
        "kmeans_labels = kmeans.fit_predict(X_reduced)\n",
        "\n",
        "# Step 5: DBSCAN for Anomaly Detection (optional, on PCA-reduced data)\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=10)\n",
        "dbscan_labels = dbscan.fit_predict(X_reduced)\n",
        "\n",
        "# Add cluster labels to the ppp_unsupervised DataFrame\n",
        "ppp_unsupervised['kmeans_cluster'] = kmeans_labels\n",
        "ppp_unsupervised['dbscan_cluster'] = dbscan_labels\n",
        "\n",
        "# Visualization of K-Means Clustering in Reduced Dimensions\n",
        "plt.figure(figsize=(8, 5))\n",
        "\n",
        "# Check if X_reduced has more than one column before plotting\n",
        "if X_reduced.shape[1] > 1:\n",
        "    plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=kmeans_labels, cmap='viridis', s=10)\n",
        "    plt.ylabel('Principal Component 2')\n",
        "else:\n",
        "    # If only one component, plot against a range of values\n",
        "    plt.scatter(X_reduced[:, 0], range(len(X_reduced)), c=kmeans_labels, cmap='viridis', s=10)\n",
        "    plt.ylabel('Data Point Index') # Update y-axis label if only one principal component\n",
        "\n",
        "plt.title(f'K-Means Clustering on PCA-Reduced PPP Unsupervised Data (K={optimal_K})')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-x3doXLE5Md-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "import seaborn as sns\n",
        "# Load original data without PCA\n",
        "X_original = ppp_unsupervised.drop(columns=['fraud'], errors='ignore')\n",
        "\n",
        "# Apply DBSCAN\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=10)\n",
        "dbscan_labels = dbscan.fit_predict(X_original)\n",
        "\n",
        "# Visualize DBSCAN clustering with pair plots or other features\n",
        "ppp_unsupervised['dbscan_cluster'] = dbscan_labels\n",
        "sns.pairplot(ppp_unsupervised, hue='dbscan_cluster', palette='viridis', plot_kws={'alpha':0.5})\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "YS0OAnmH55Ss"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}