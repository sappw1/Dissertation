{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/sappw1/Dissertation/blob/main/Notebooks/Notebooks-Working/ETL/prac_scraper.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjZ_2Mkw01a_"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import json\n",
        "\n",
        "base_url = \"https://pandemicoversight.gov\"\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0 Safari/537.36'\n",
        "}\n",
        "\n",
        "def safe_request(url):\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        return response\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Request error: {e} - URL: {url}\")\n",
        "        return None\n",
        "\n",
        "def get_reports(page_number):\n",
        "    url = f\"https://pandemicoversight.gov/oversight/reports?f%5B0%5D=report_type_taxonomy%3A85&page={page_number}\"\n",
        "    response = safe_request(url)\n",
        "    if not response:\n",
        "        return []\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    reports = []\n",
        "\n",
        "    for item in soup.select('.views-row'):\n",
        "        title_tag = item.select_one('.display__condensed--title a')\n",
        "        date_tag = item.select_one('.display__condensed--footer time')\n",
        "\n",
        "        if title_tag and date_tag:\n",
        "            title = title_tag.text.strip()\n",
        "            date = date_tag.text.strip()\n",
        "            link = title_tag['href']\n",
        "\n",
        "            reports.append({\n",
        "                'title': title,\n",
        "                'date': date,\n",
        "                'link': link\n",
        "            })\n",
        "\n",
        "    return reports\n",
        "\n",
        "def get_press_release(url):\n",
        "    response = safe_request(url)\n",
        "    if not response:\n",
        "        return \"\"\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    content = soup.select_one('.node-body .field_body')\n",
        "\n",
        "    return content.text.strip() if content else \"\"\n",
        "\n",
        "# Crawl through pages with optional limit\n",
        "all_reports = []\n",
        "page_limit = None  # Set limit here for testing, None for no limit\n",
        "page_count = 0\n",
        "\n",
        "while page_limit is None or page_count < page_limit:\n",
        "    print(f\"Scraping page: {page_count + 1}\")\n",
        "    reports = get_reports(page_count)\n",
        "\n",
        "    if not reports:\n",
        "        break\n",
        "\n",
        "    for report in reports:\n",
        "        print(f\"Fetching press release for: {report['title']}\")\n",
        "        full_url = report['link'] if report['link'].startswith('http') else base_url + report['link']\n",
        "        report['press_release'] = get_press_release(full_url)\n",
        "        time.sleep(1)  # delay between press release requests\n",
        "\n",
        "    all_reports.extend(reports)\n",
        "    page_count += 1\n",
        "\n",
        "    # Incremental saving\n",
        "    with open('pandemic_reports.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(all_reports, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    time.sleep(2)  # delay between page requests\n",
        "\n",
        "# Display results\n",
        "for report in all_reports:\n",
        "    print(f\"Title: {report['title']}\\nDate: {report['date']}\\nLink: {report['link']}\\nPress Release:\\n{report['press_release']}\\n{'-'*80}\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyM3n/Ahjnfe0gYj7bXNJybo",
      "gpuType": "L4",
      "include_colab_link": true,
      "machine_shape": "hm",
      "mount_file_id": "1z0_MwBnFYmw86Z964llM0xVceG0jmuEi",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
