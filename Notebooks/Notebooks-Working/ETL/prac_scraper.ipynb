{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "mount_file_id": "1z0_MwBnFYmw86Z964llM0xVceG0jmuEi",
      "authorship_tag": "ABX9TyOhBKLt/6SNAvi62Mr5qIDw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sappw1/Dissertation/blob/main/Notebooks/Notebooks-Working/ETL/prac_scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import json\n",
        "\n",
        "base_url = \"https://pandemicoversight.gov\"\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0 Safari/537.36'\n",
        "}\n",
        "\n",
        "def safe_request(url):\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        return response\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Request error: {e} - URL: {url}\")\n",
        "        return None\n",
        "\n",
        "def get_reports(page_number):\n",
        "    url = f\"https://pandemicoversight.gov/oversight/reports?f%5B0%5D=report_type_taxonomy%3A85&page={page_number}\"\n",
        "    response = safe_request(url)\n",
        "    if not response:\n",
        "        return []\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    reports = []\n",
        "\n",
        "    for item in soup.select('.views-row'):\n",
        "        title_tag = item.select_one('.display__condensed--title a')\n",
        "        date_tag = item.select_one('.display__condensed--footer time')\n",
        "\n",
        "        if title_tag and date_tag:\n",
        "            title = title_tag.text.strip()\n",
        "            date = date_tag.text.strip()\n",
        "            link = title_tag['href']\n",
        "\n",
        "            reports.append({\n",
        "                'title': title,\n",
        "                'date': date,\n",
        "                'link': link\n",
        "            })\n",
        "\n",
        "    return reports\n",
        "\n",
        "def get_press_release(url):\n",
        "    response = safe_request(url)\n",
        "    if not response:\n",
        "        return \"\"\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    content = soup.select_one('.node-body .field_body')\n",
        "\n",
        "    return content.text.strip() if content else \"\"\n",
        "\n",
        "# Crawl through pages with optional limit\n",
        "all_reports = []\n",
        "page_limit = None  # Set limit here for testing, None for no limit\n",
        "page_count = 0\n",
        "\n",
        "while page_limit is None or page_count < page_limit:\n",
        "    print(f\"Scraping page: {page_count + 1}\")\n",
        "    reports = get_reports(page_count)\n",
        "\n",
        "    if not reports:\n",
        "        break\n",
        "\n",
        "    for report in reports:\n",
        "        print(f\"Fetching press release for: {report['title']}\")\n",
        "        full_url = report['link'] if report['link'].startswith('http') else base_url + report['link']\n",
        "        report['press_release'] = get_press_release(full_url)\n",
        "        time.sleep(1)  # delay between press release requests\n",
        "\n",
        "    all_reports.extend(reports)\n",
        "    page_count += 1\n",
        "\n",
        "    # Incremental saving\n",
        "    with open('pandemic_reports.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(all_reports, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    time.sleep(2)  # delay between page requests\n",
        "\n",
        "# Display results\n",
        "for report in all_reports:\n",
        "    print(f\"Title: {report['title']}\\nDate: {report['date']}\\nLink: {report['link']}\\nPress Release:\\n{report['press_release']}\\n{'-'*80}\\n\")\n"
      ],
      "metadata": {
        "id": "wjZ_2Mkw01a_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "import re\n",
        "from rapidfuzz import fuzz, process\n",
        "import json\n",
        "\n",
        "# Load spaCy NLP model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Improved regex for monetary amounts\n",
        "MONEY_REGEX = r\"\\$[\\d,.]+(?:\\s?(million|billion|thousand|k|m|b))?\"\n",
        "\n",
        "def normalize_entity(entity):\n",
        "    if isinstance(entity, str):\n",
        "        entity = entity.lower().replace(\".\", \"\").strip()\n",
        "        entity = re.sub(r\"\\s+(llc|inc|corp|ltd)\\.?$\", \"\", entity)\n",
        "    else:\n",
        "        entity = \"\"\n",
        "    return entity\n",
        "\n",
        "def parse_money(amount_str):\n",
        "    multipliers = {'thousand':1e3, 'million':1e6, 'billion':1e9, 'k':1e3, 'm':1e6, 'b':1e9}\n",
        "    amount_str = amount_str.lower().replace(\",\", \"\").replace(\"$\", \"\").strip()\n",
        "    match = re.match(r\"([\\d.]+)\\s?(thousand|million|billion|k|m|b)?\", amount_str)\n",
        "    if match:\n",
        "        number = float(match.group(1))\n",
        "        multiplier = multipliers.get(match.group(2), 1)\n",
        "        return number * multiplier\n",
        "    return None\n",
        "\n",
        "def extract_entities(text):\n",
        "    doc = nlp(text)\n",
        "    entities = {\n",
        "        \"names\": list({normalize_entity(ent.text) for ent in doc.ents if ent.label_ in [\"PERSON\", \"ORG\"]}),\n",
        "        \"locations\": list({ent.text for ent in doc.ents if ent.label_ in [\"GPE\", \"LOC\"]}),\n",
        "        \"dates\": list({ent.text for ent in doc.ents if ent.label_ == \"DATE\" and re.search(r\"\\d{4}\", ent.text)}),\n",
        "        \"money\": list({ent.text for ent in doc.ents if ent.label_ == \"MONEY\"})\n",
        "    }\n",
        "    loan_amounts = re.findall(MONEY_REGEX, text)\n",
        "    entities[\"loan_amounts\"] = [parse_money(amt[0] if isinstance(amt, tuple) else amt) for amt in loan_amounts]\n",
        "    entities[\"loan_amounts\"] = [amt for amt in entities[\"loan_amounts\"] if amt]\n",
        "    return entities\n",
        "\n",
        "# Load press releases from JSON\n",
        "with open(\"/content/drive/MyDrive/NCU/Dissertation/Data/pandemic_reports.json\", \"r\") as file:\n",
        "    press_releases = json.load(file)\n",
        "\n",
        "# Load loan applications data\n",
        "loan_df = pd.read_csv(\"/content/drive/MyDrive/NCU/Dissertation/Data/PPP_Loan_apps.csv\")\n",
        "loan_df[\"NormalizedBorrowerName\"] = loan_df[\"BorrowerName\"].apply(normalize_entity)\n",
        "\n",
        "matched_results = []\n",
        "\n",
        "for release in press_releases:\n",
        "    content = release[\"press_release\"]\n",
        "    entities = extract_entities(content)\n",
        "\n",
        "    matched_app = None\n",
        "\n",
        "    for name in entities[\"names\"]:\n",
        "        match_name, score, idx = process.extractOne(name, loan_df[\"NormalizedBorrowerName\"], scorer=fuzz.token_sort_ratio)\n",
        "        if score >= 85:\n",
        "            potential_match = loan_df.iloc[idx]\n",
        "\n",
        "            # Loan amount matching\n",
        "            amount_matched = False\n",
        "            for amount in entities[\"loan_amounts\"]:\n",
        "                if abs(amount - potential_match[\"CurrentApprovalAmount\"]) < 1000:\n",
        "                    amount_matched = True\n",
        "                    break\n",
        "\n",
        "            if amount_matched:\n",
        "                matched_app = potential_match\n",
        "                break\n",
        "\n",
        "    if matched_app is not None:\n",
        "        result = {\n",
        "            \"Title\": release[\"title\"],\n",
        "            \"Date\": release[\"date\"],\n",
        "            \"Link\": release[\"link\"],\n",
        "            \"MatchedLoanNumber\": matched_app[\"LoanNumber\"],\n",
        "            \"MatchedBorrowerName\": matched_app[\"BorrowerName\"],\n",
        "            \"LoanAmount\": matched_app[\"CurrentApprovalAmount\"],\n",
        "            \"LoanApprovalDate\": matched_app[\"DateApproved\"],\n",
        "            \"is_fraudulent\": 1\n",
        "        }\n",
        "        matched_results.append(result)\n",
        "\n",
        "# Save matched results\n",
        "matched_df = pd.DataFrame(matched_results)\n",
        "matched_df.to_csv(\"content/drive/MyDrive/NCU/Dissertation/Data/matched_fraud_cases_22mar25.csv\", index=False)\n",
        "matched_df.to_json(\"content/drive/MyDrive/NCU/Dissertation/Data/matched_fraud_cases_22mar25.json\", orient=\"records\", indent=4)\n",
        "\n",
        "print(f\"Matched {len(matched_results)} fraud cases saved.\")\n"
      ],
      "metadata": {
        "id": "ixCJr5aClhY4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "53c8d4f4-1e60-4206-f968-f5ead07ef858"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "Cannot save file into a non-existent directory: 'content/drive/MyDrive/NCU/Dissertation/Data'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-df14629df4a3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;31m# Save matched results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0mmatched_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatched_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m \u001b[0mmatched_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"content/drive/MyDrive/NCU/Dissertation/Data/matched_fraud_cases_22mar25.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0mmatched_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"content/drive/MyDrive/NCU/Dissertation/Data/matched_fraud_cases_22mar25.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"records\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                 )\n\u001b[0;32m--> 333\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3965\u001b[0m         )\n\u001b[1;32m   3966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3967\u001b[0;31m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[1;32m   3968\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3969\u001b[0m             \u001b[0mlineterminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlineterminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1012\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         )\n\u001b[0;32m-> 1014\u001b[0;31m         \u001b[0mcsv_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \"\"\"\n\u001b[1;32m    250\u001b[0m         \u001b[0;31m# apply compression and byte/text conversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         with get_handle(\n\u001b[0m\u001b[1;32m    252\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[0;31m# Only for write methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"r\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m         \u001b[0mcheck_parent_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    614\u001b[0m     \u001b[0mparent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 616\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mrf\"Cannot save file into a non-existent directory: '{parent}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'content/drive/MyDrive/NCU/Dissertation/Data'"
          ]
        }
      ]
    },
    {
      "source": [
        "!pip install rapidfuzz"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHc09lSmoYVM",
        "outputId": "e8ac1fee-cdff-4128-b8b4-8490fd7823d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rapidfuzz\n",
            "  Downloading rapidfuzz-3.12.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Downloading rapidfuzz-3.12.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m158.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz\n",
            "Successfully installed rapidfuzz-3.12.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reports"
      ],
      "metadata": {
        "id": "4495jE0Bigcv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}