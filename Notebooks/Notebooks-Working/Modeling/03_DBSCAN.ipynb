{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8baab527-034b-44c6-8a9b-8231b50c72ef",
      "metadata": {
        "vscode": {
          "languageId": "python"
        },
        "id": "8baab527-034b-44c6-8a9b-8231b50c72ef"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import numpy as np\n",
        "import cupy as cp\n",
        "import pandas as pd\n",
        "import cudf\n",
        "from cuml import DBSCAN\n",
        "from cuml.metrics.cluster.silhouette_score import cython_silhouette_score\n",
        "from sklearn.metrics import davies_bouldin_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b002441e",
      "metadata": {
        "vscode": {
          "languageId": "python"
        },
        "id": "b002441e"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def run_dbscan_on_all(pca_data, output_dir, min_samples=5, eps_list=None):\n",
        "    if eps_list is None:\n",
        "        eps_list = [0.3, 0.5, 0.7, 1.0, 1.3]\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    results_path = os.path.join(output_dir, \"dbscan_results.json\")\n",
        "\n",
        "    # Load existing results to avoid overwriting\n",
        "    if os.path.exists(results_path):\n",
        "        with open(results_path, \"r\") as f:\n",
        "            results = json.load(f)\n",
        "    else:\n",
        "        results = {}\n",
        "\n",
        "    for name, X in pca_data.items():\n",
        "        print(f\"\\n Running DBSCAN on: {name}\")\n",
        "        if name not in results:\n",
        "            results[name] = {}\n",
        "        for eps in eps_list:\n",
        "            print(f\"  • eps = {eps}\")\n",
        "            model = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "            labels = model.fit_predict(X).to_numpy()\n",
        "            n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "            n_noise = np.sum(labels == -1)\n",
        "\n",
        "            metrics = {\n",
        "                \"n_clusters\": int(n_clusters),\n",
        "                \"n_noise\": int(n_noise),\n",
        "                \"eps\": float(eps)\n",
        "            }\n",
        "\n",
        "            # Auto-skip silhouette/DBI if dataset is large or cluster structure is invalid\n",
        "            if 1 < n_clusters < 50 and len(labels) < 300000:\n",
        "                try:\n",
        "                    sil = float(cython_silhouette_score(X, labels, metric='euclidean'))\n",
        "                    dbi = davies_bouldin_score(X.to_numpy(), labels)\n",
        "                    metrics[\"silhouette\"] = sil\n",
        "                    metrics[\"dbi\"] = dbi\n",
        "                except Exception as e:\n",
        "                    print(f\"     Skipped metrics (runtime error): {e}\")\n",
        "                    metrics[\"silhouette\"] = None\n",
        "                    metrics[\"dbi\"] = None\n",
        "            else:\n",
        "                print(f\"     Skipped metrics due to size or structure: n={len(labels)}, clusters={n_clusters}\")\n",
        "\n",
        "            # Save cluster labels\n",
        "            label_filename = f\"labels_dbscan_{name}_eps{str(eps).replace('.', '')}.npy\"\n",
        "            np.save(os.path.join(output_dir, label_filename), labels)\n",
        "\n",
        "            results[name][str(eps)] = metrics\n",
        "\n",
        "            # Save JSON\n",
        "            with open(os.path.join(output_dir, \"dbscan_results.json\"), \"w\") as f:\n",
        "                json.dump(results, f, indent=4)\n",
        "\n",
        "    print(\"\\n All DBSCAN runs complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6409893",
      "metadata": {
        "vscode": {
          "languageId": "python"
        },
        "id": "d6409893",
        "outputId": "eb9721e8-ef7c-4ccd-f378-1199b681f0d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            " Selected PCA arrays loaded for DBSCAN.\n",
            "\n",
            " Running DBSCAN on: Key (3C)\n",
            "  • eps = 0.3\n",
            "[2025-05-14 23:54:36.607] [CUML] [warning] Batch size limited by the chosen integer type (4 bytes). 7010 -> 2217. Using the larger integer type might result in better performance\n",
            "     Skipped metrics due to size or structure: n=968524, clusters=5491\n",
            "  • eps = 0.5\n",
            "[2025-05-14 23:55:03.823] [CUML] [warning] Batch size limited by the chosen integer type (4 bytes). 7010 -> 2217. Using the larger integer type might result in better performance\n",
            "     Skipped metrics due to size or structure: n=968524, clusters=5074\n",
            "  • eps = 0.7\n",
            "[2025-05-14 23:55:35.977] [CUML] [warning] Batch size limited by the chosen integer type (4 bytes). 7010 -> 2217. Using the larger integer type might result in better performance\n",
            "     Skipped metrics due to size or structure: n=968524, clusters=4933\n",
            "  • eps = 1.0\n",
            "[2025-05-14 23:56:10.805] [CUML] [warning] Batch size limited by the chosen integer type (4 bytes). 7010 -> 2217. Using the larger integer type might result in better performance\n",
            "     Skipped metrics due to size or structure: n=968524, clusters=1072\n",
            "  • eps = 1.3\n",
            "[2025-05-14 23:57:02.474] [CUML] [warning] Batch size limited by the chosen integer type (4 bytes). 7010 -> 2217. Using the larger integer type might result in better performance\n",
            "     Skipped metrics due to size or structure: n=968524, clusters=416\n",
            "\n",
            " All DBSCAN runs complete.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import cupy as cp\n",
        "import cudf\n",
        "\n",
        "#  Mount your Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#  Paths\n",
        "input_dir = \"/content/drive/MyDrive/NCU/Dissertation/Data/Processed/PCA_Arrays\"\n",
        "output_dir = \"/content/drive/MyDrive/NCU/Dissertation/Data/Processed/Clustering/DBSCAN\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "#  Load only the 2C and 3C PCA projections (95% skipped)\n",
        "#X_all_pca_2 = cp.load(os.path.join(input_dir, \"X_all_pca_2.npy\"))\n",
        "#X_all_pca_3 = cp.load(os.path.join(input_dir, \"X_all_pca_3.npy\"))\n",
        "#X_key_pca_2 = cp.load(os.path.join(input_dir, \"X_key_pca_2.npy\"))\n",
        "X_key_pca_3 = cp.load(os.path.join(input_dir, \"X_key_pca_3.npy\"))\n",
        "\n",
        "print(\" Selected PCA arrays loaded for DBSCAN.\")\n",
        "\n",
        "# Convert CuPy arrays to cuDF DataFrames\n",
        "def to_cudf(cp_array):\n",
        "    return cudf.DataFrame(cp_array)\n",
        "\n",
        "#  Dictionary for DBSCAN\n",
        "#dbscan_inputs = {\n",
        "#    \"Full (2C)\": to_cudf(X_all_pca_2),\n",
        "#    \"Full (3C)\": to_cudf(X_all_pca_3),\n",
        "#    \"Key (2C)\": to_cudf(X_key_pca_2),\n",
        "#    \"Key (3C)\": to_cudf(X_key_pca_3),\n",
        "#}\n",
        "\n",
        "#full_configs = {\n",
        "#    \"Full (2C)\": to_cudf(X_all_pca_2),\n",
        "#    \"Full (3C)\": to_cudf(X_all_pca_3)\n",
        "#}\n",
        "\n",
        "#run_dbscan_on_all(full_configs, output_dir, min_samples=5)\n",
        "\n",
        "key_configs = {\n",
        " #   \"Key (2C)\": to_cudf(X_key_pca_2)\n",
        "    \"Key (3C)\": to_cudf(X_key_pca_3)\n",
        "}\n",
        "run_dbscan_on_all(key_configs, output_dir, min_samples=5)\n",
        "\n",
        "\n",
        "\n",
        "# Run DBSCAN clustering on all configs\n",
        "#run_dbscan_on_all(dbscan_inputs, output_dir, min_samples=5)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def load_dbscan_visualization_data(base_dir, include_metrics=True):\n",
        "    \"\"\"\n",
        "    Load PCA projections, DBSCAN labels, and fraud labels for visualization.\n",
        "    Directory structure assumed:\n",
        "    - base_dir/\n",
        "        ├── PCA/\n",
        "        ├── DBSCAN/\n",
        "        └── y_labels.pkl\n",
        "    \"\"\"\n",
        "    pca_dir = os.path.join(base_dir, \"PCA\")\n",
        "    db_dir = os.path.join(base_dir, \"DBSCAN\")\n",
        "\n",
        "    data = {}\n",
        "\n",
        "    # Load fraud labels\n",
        "    y_path = os.path.join(base_dir, \"y_labels.pkl\")\n",
        "    y = pd.read_pickle(y_path)\n",
        "\n",
        "    index_all = pd.read_csv(os.path.join(base_dir, \"index_all_scaled.csv\"), index_col=0).index\n",
        "    index_key = pd.read_csv(os.path.join(base_dir, \"index_key_scaled.csv\"), index_col=0).index\n",
        "\n",
        "    data[\"y_all\"] = y.loc[index_all].values\n",
        "    data[\"y_key\"] = y.loc[index_key].values\n",
        "\n",
        "    data[\"index_all\"] = index_all\n",
        "    data[\"index_key\"] = index_key\n",
        "\n",
        "    # Load PCA arrays\n",
        "    for fname in os.listdir(pca_dir):\n",
        "        if fname.endswith(\".npy\"):\n",
        "            key = fname.replace(\".npy\", \"\")\n",
        "            data[key] = np.load(os.path.join(pca_dir, fname))\n",
        "\n",
        "    # Load DBSCAN labels\n",
        "    for fname in os.listdir(db_dir):\n",
        "        if fname.startswith(\"labels_dbscan\") and fname.endswith(\".npy\"):\n",
        "            key = fname.replace(\".npy\", \"\")\n",
        "            data[key] = np.load(os.path.join(db_dir, fname))\n",
        "\n",
        "    # Load DBSCAN metrics\n",
        "    if include_metrics:\n",
        "        metrics_path = os.path.join(db_dir, \"dbscan_results.json\")\n",
        "        if os.path.exists(metrics_path):\n",
        "            with open(metrics_path, \"r\") as f:\n",
        "                data[\"dbscan_results\"] = json.load(f)\n",
        "\n",
        "    return data\n"
      ],
      "metadata": {
        "id": "ic_XW33_8rkD"
      },
      "id": "ic_XW33_8rkD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_dbscan_comparison(x, y_true, label_dict, label_display, eps_mapping, dbscan_results, save_path=None):\n",
        "    fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(16, 8), constrained_layout=True)\n",
        "    axs = axs.flatten()\n",
        "\n",
        "    for i, (eps_str, labels) in enumerate(label_dict.items()):\n",
        "        epsilon_val = eps_mapping[eps_str]\n",
        "        metrics = dbscan_results.get(label_display, {}).get(str(epsilon_val), {})\n",
        "        k_val = metrics.get(\"n_clusters\", \"?\")\n",
        "\n",
        "        axs[i].scatter(x[:, 0], x[:, 1], c=labels, cmap=\"cividis\", alpha=0.6, s=10)\n",
        "        axs[i].scatter(x[y_true == 1][:, 0], x[y_true == 1][:, 1], c=\"red\", marker=\"x\", s=10, label=\"Fraud\")\n",
        "        axs[i].scatter(x[labels == -1][:, 0], x[labels == -1][:, 1], c=\"black\", marker=\"v\", s=10, label=\"Noise\")\n",
        "        axs[i].set_title(f\"ε = {epsilon_val:.2f}, k = {k_val}\")\n",
        "        axs[i].set_xlabel(\"PC1\")\n",
        "        axs[i].set_ylabel(\"PC2\")\n",
        "\n",
        "    # 6th panel (bottom right) for legend only\n",
        "    axs[-1].axis(\"off\")\n",
        "    handles, labels = axs[0].get_legend_handles_labels()\n",
        "    axs[-1].legend(handles, labels, loc=\"center\", fontsize=\"medium\")\n",
        "\n",
        "    fig.suptitle(f\"DBSCAN Results for {label_display}\", fontsize=16)\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "nvqR0vP08tjC"
      },
      "id": "nvqR0vP08tjC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Config map: (PCA key, fraud label key, list of eps values to visualize)\n",
        "dbscan_config_map = {\n",
        "    \"Full (2C)\": (\"x_all_pca_2\", \"y_all\", [0.3, 0.5, 0.7, 1.0, 1.3]),\n",
        "    \"Full (3C)\": (\"x_all_pca_3\", \"y_all\", [0.3, 0.5, 0.7, 1.0, 1.3]),\n",
        "    \"Key (2C)\": (\"x_key_pca_2\", \"y_key\", [0.3, 0.5, 0.7, 1.0, 1.3]),\n",
        "    \"Key (3C)\": (\"x_key_pca_3\", \"y_key\", [0.3, 0.5, 0.7, 1.0, 1.3])\n",
        "}\n"
      ],
      "metadata": {
        "id": "zL-DZz2Q8vWz"
      },
      "id": "zL-DZz2Q8vWz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set base path and output dir\n",
        "base_dir = \"/content/drive/MyDrive/NCU/Dissertation/Data/Processed/Clustering\"\n",
        "output_dir = \"/content/drive/MyDrive/NCU/Dissertation/Figures/DBSCANPlots\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Load all data\n",
        "data = load_dbscan_visualization_data(base_dir)\n",
        "\n",
        "# Generate plots\n",
        "for label, (x_key, y_key, eps_list) in dbscan_config_map.items():\n",
        "    x = data[x_key]\n",
        "    y = data[y_key]\n",
        "\n",
        "    eps_mapping = {f\"{int(eps * 10):02d}\": eps for eps in eps_list}\n",
        "    label_dict = {}\n",
        "\n",
        "    for eps in eps_list:\n",
        "        eps_str = f\"{int(eps * 10):02d}\"\n",
        "        file_key = f\"labels_dbscan_{label}_eps{eps_str}\"\n",
        "        if file_key in data:\n",
        "            label_dict[eps_str] = data[file_key]\n",
        "        else:\n",
        "            print(f\"⚠️  Missing: {file_key}\")\n",
        "\n",
        "    if label_dict:\n",
        "        save_path = os.path.join(\n",
        "            output_dir,\n",
        "            f\"dbscan_comparison_{label.lower().replace(' ', '_').replace('(', '').replace(')', '')}.png\"\n",
        "        )\n",
        "        print(f\"✅ Generating: {label} with ε = {list(eps_mapping.values())}\")\n",
        "        plot_dbscan_comparison(\n",
        "            x=x,\n",
        "            y_true=y,\n",
        "            label_dict=label_dict,\n",
        "            label_display=label,\n",
        "            eps_mapping=eps_mapping,\n",
        "            dbscan_results=data[\"dbscan_results\"],\n",
        "            save_path=save_path\n",
        "        )\n"
      ],
      "metadata": {
        "id": "TICexj1Q8xFB"
      },
      "id": "TICexj1Q8xFB",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "",
      "name": ""
    },
    "language_info": {
      "name": ""
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}