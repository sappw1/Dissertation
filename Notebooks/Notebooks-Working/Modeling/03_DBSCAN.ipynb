{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8baab527-034b-44c6-8a9b-8231b50c72ef",
      "metadata": {
        "vscode": {
          "languageId": "python"
        },
        "id": "8baab527-034b-44c6-8a9b-8231b50c72ef"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import numpy as np\n",
        "import cupy as cp\n",
        "import pandas as pd\n",
        "import cudf\n",
        "from cuml import DBSCAN\n",
        "from cuml.metrics.cluster.silhouette_score import cython_silhouette_score\n",
        "from sklearn.metrics import davies_bouldin_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b002441e",
      "metadata": {
        "vscode": {
          "languageId": "python"
        },
        "id": "b002441e"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def run_dbscan_on_all(pca_data, output_dir, min_samples=5, eps_list=None):\n",
        "    if eps_list is None:\n",
        "        eps_list = [0.3, 0.5, 0.7, 1.0, 1.3]\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    results_path = os.path.join(output_dir, \"dbscan_results.json\")\n",
        "\n",
        "    # Load existing results to avoid overwriting\n",
        "    if os.path.exists(results_path):\n",
        "        with open(results_path, \"r\") as f:\n",
        "            results = json.load(f)\n",
        "    else:\n",
        "        results = {}\n",
        "\n",
        "    for name, X in pca_data.items():\n",
        "        print(f\"\\n Running DBSCAN on: {name}\")\n",
        "        if name not in results:\n",
        "            results[name] = {}\n",
        "        for eps in eps_list:\n",
        "            print(f\"  • eps = {eps}\")\n",
        "            model = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "            labels = model.fit_predict(X).to_numpy()\n",
        "            n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "            n_noise = np.sum(labels == -1)\n",
        "\n",
        "            metrics = {\n",
        "                \"n_clusters\": int(n_clusters),\n",
        "                \"n_noise\": int(n_noise),\n",
        "                \"eps\": float(eps)\n",
        "            }\n",
        "\n",
        "            # Auto-skip silhouette/DBI if dataset is large or cluster structure is invalid\n",
        "            if 1 < n_clusters < 50 and len(labels) < 300000:\n",
        "                try:\n",
        "                    sil = float(cython_silhouette_score(X, labels, metric='euclidean'))\n",
        "                    dbi = davies_bouldin_score(X.to_numpy(), labels)\n",
        "                    metrics[\"silhouette\"] = sil\n",
        "                    metrics[\"dbi\"] = dbi\n",
        "                except Exception as e:\n",
        "                    print(f\"     Skipped metrics (runtime error): {e}\")\n",
        "                    metrics[\"silhouette\"] = None\n",
        "                    metrics[\"dbi\"] = None\n",
        "            else:\n",
        "                print(f\"     Skipped metrics due to size or structure: n={len(labels)}, clusters={n_clusters}\")\n",
        "\n",
        "            # Save cluster labels\n",
        "            label_filename = f\"labels_dbscan_{name}_eps{str(eps).replace('.', '')}.npy\"\n",
        "            np.save(os.path.join(output_dir, label_filename), labels)\n",
        "\n",
        "            results[name][str(eps)] = metrics\n",
        "\n",
        "            # Save JSON\n",
        "            with open(os.path.join(output_dir, \"dbscan_results.json\"), \"w\") as f:\n",
        "                json.dump(results, f, indent=4)\n",
        "\n",
        "    print(\"\\n All DBSCAN runs complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d6409893",
      "metadata": {
        "vscode": {
          "languageId": "python"
        },
        "id": "d6409893",
        "outputId": "eb9721e8-ef7c-4ccd-f378-1199b681f0d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            " Selected PCA arrays loaded for DBSCAN.\n",
            "\n",
            " Running DBSCAN on: Key (3C)\n",
            "  • eps = 0.3\n",
            "[2025-05-14 23:54:36.607] [CUML] [warning] Batch size limited by the chosen integer type (4 bytes). 7010 -> 2217. Using the larger integer type might result in better performance\n",
            "     Skipped metrics due to size or structure: n=968524, clusters=5491\n",
            "  • eps = 0.5\n",
            "[2025-05-14 23:55:03.823] [CUML] [warning] Batch size limited by the chosen integer type (4 bytes). 7010 -> 2217. Using the larger integer type might result in better performance\n",
            "     Skipped metrics due to size or structure: n=968524, clusters=5074\n",
            "  • eps = 0.7\n",
            "[2025-05-14 23:55:35.977] [CUML] [warning] Batch size limited by the chosen integer type (4 bytes). 7010 -> 2217. Using the larger integer type might result in better performance\n",
            "     Skipped metrics due to size or structure: n=968524, clusters=4933\n",
            "  • eps = 1.0\n",
            "[2025-05-14 23:56:10.805] [CUML] [warning] Batch size limited by the chosen integer type (4 bytes). 7010 -> 2217. Using the larger integer type might result in better performance\n",
            "     Skipped metrics due to size or structure: n=968524, clusters=1072\n",
            "  • eps = 1.3\n",
            "[2025-05-14 23:57:02.474] [CUML] [warning] Batch size limited by the chosen integer type (4 bytes). 7010 -> 2217. Using the larger integer type might result in better performance\n",
            "     Skipped metrics due to size or structure: n=968524, clusters=416\n",
            "\n",
            " All DBSCAN runs complete.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import cupy as cp\n",
        "import cudf\n",
        "\n",
        "#  Mount your Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#  Paths\n",
        "input_dir = \"/content/drive/MyDrive/NCU/Dissertation/Data/Processed/PCA_Arrays\"\n",
        "output_dir = \"/content/drive/MyDrive/NCU/Dissertation/Data/Processed/Clustering/DBSCAN\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "#  Load only the 2C and 3C PCA projections (95% skipped)\n",
        "#X_all_pca_2 = cp.load(os.path.join(input_dir, \"X_all_pca_2.npy\"))\n",
        "#X_all_pca_3 = cp.load(os.path.join(input_dir, \"X_all_pca_3.npy\"))\n",
        "#X_key_pca_2 = cp.load(os.path.join(input_dir, \"X_key_pca_2.npy\"))\n",
        "X_key_pca_3 = cp.load(os.path.join(input_dir, \"X_key_pca_3.npy\"))\n",
        "\n",
        "print(\" Selected PCA arrays loaded for DBSCAN.\")\n",
        "\n",
        "# Convert CuPy arrays to cuDF DataFrames\n",
        "def to_cudf(cp_array):\n",
        "    return cudf.DataFrame(cp_array)\n",
        "\n",
        "#  Dictionary for DBSCAN\n",
        "#dbscan_inputs = {\n",
        "#    \"Full (2C)\": to_cudf(X_all_pca_2),\n",
        "#    \"Full (3C)\": to_cudf(X_all_pca_3),\n",
        "#    \"Key (2C)\": to_cudf(X_key_pca_2),\n",
        "#    \"Key (3C)\": to_cudf(X_key_pca_3),\n",
        "#}\n",
        "\n",
        "#full_configs = {\n",
        "#    \"Full (2C)\": to_cudf(X_all_pca_2),\n",
        "#    \"Full (3C)\": to_cudf(X_all_pca_3)\n",
        "#}\n",
        "\n",
        "#run_dbscan_on_all(full_configs, output_dir, min_samples=5)\n",
        "\n",
        "key_configs = {\n",
        " #   \"Key (2C)\": to_cudf(X_key_pca_2)\n",
        "    \"Key (3C)\": to_cudf(X_key_pca_3)\n",
        "}\n",
        "run_dbscan_on_all(key_configs, output_dir, min_samples=5)\n",
        "\n",
        "\n",
        "\n",
        "# Run DBSCAN clustering on all configs\n",
        "#run_dbscan_on_all(dbscan_inputs, output_dir, min_samples=5)\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "",
      "name": ""
    },
    "language_info": {
      "name": ""
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}